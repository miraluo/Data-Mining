---
title: "HW4"
author: "YAYUAN LUO"
date: "4/10/2022"
output: html_document
---
Task 1: Classification Tree
1.Bring the dataset people_analytics.csv into your R environment
```{r }
library(ggplot2)
library(tidyverse)
library(lubridate)
library(dplyr)
people<-read.csv('/Users/luoyayuan/Desktop/Class Slides/699/assignments/assignment4/people_analytics.csv')
str(people)
View(people)
```


2. Using your assigned seed value (from Assignment 2), partition your data into training (60%) and validation (40%) sets. Show the step(s) that you used to do this.
```{r }
set.seed(180)
train<-sample_frac(people,0.6)
valid<-setdiff(people,train)
```


3. Build a tree model with this dataset, using Attrition as your outcome variable.
```{r }
library(rpart)
library(rpart.plot)
model<-rpart(Attrition~.,data=train,method = 'class')
sum(is.na(train))
```


4. Use rpart.plot to display a classification tree that depicts your model.
```{r }
rpart.plot(model,main='Tree model')
```

```{r }
rpart.plot(model,type=5,extra=6,main='Tree model')
```

```{r }
#palette() 
rpart.plot(model,cex=0.5,type=2,extra=5,fallen=1,varlen=2,main='Tree model',box.palette = "#61D04F")
```
We change some parameters to adjust the tree model. The first model shows the overall root node's proportion first and assigns yes and no proportion to the next node. The second model only shows each node without indicating the proportion. The third one shows the overall root node's yes and no choice proportions in the same oval frame， which is a little bit confusing about which proportion directs to yes and which proportion directs to no. Thus I prefer the second one since it is simple and clear showing the results.


5. Describe the split that’s created at your tree’s root node (what variable did it split on, and what rule did it use?). Why is the root node significant?

tree’s root nodes split into sub-nodes and this process continues until homogeneous nodes are left. From my tree, it judges whether it is over time or not. If it is over time it goes to the left while if it is not over time it goes to the right. Every node individually calculates the entropy of each child node. Calculate the entropy of each split as the weighted average entropy of child nodes. Select the split with the lowest entropy or highest information gain.
It is important because tree model evaluates the variable that can best split our data. The root node is the highest node in the tree structure, and has no parent. This node is a global element and represents the entire message.


6. Did all the input variables from the dataset appear in your model diagram? If not, why not?

Not all input variables from the dataset appear in my model diagram. This is exactly how the tree model is fantastic. It helps us eliminate some not very important variables and keeps key factors here to make further prediction. We have root node and split nodes that indicates the most important features for us to focus. Also tree model with less variables to some extent reduce overfitting risks.



7. Describe any one rule that your tree generates regarding whether an employee will leave the company. To describe a rule, just trace any path along your tree from the root node to a terminal node.

I will describe my first tree model. Since my output variable is attrition and we want to see whether an employee will leave the company, we want the terminal node shows 'yes' result. If this employee works overtime with joblevel large or equal to 2, his distance from home is large or equal to 7 and his monthly income is large or equal to 2537, it has 90% chances that he will leave the company. 



8. Now, build another tree model. This time, set a complexity parameter of 0, and use minsplit =2, to make the tree as large as possible. Show what your overfit tree looks like, using rpart.plot. 
```{r }
model1 <- rpart(Attrition ~ ., data = train,method = "class",cp = 0,minsplit =2)
rpart.plot(model1,main='Tree model')
```


9. Using five-fold cross-validation, determine the optimal complexity parameter (cp) for a tree model built with your training data. 
```{r }
model2 <- rpart(Attrition ~ .,data = train,method = "class", xval=5, minsplit =2,cp = 0)
cros<-printcp(model2)
class(cros)
plotcp(model2 )

# I will choose cp = 4 since this cp with lowest xerror.
```


10. Generate a new tree model, with the cp value that you found previously.
```{r }
model3 <- rpart(Attrition ~ .,data = train,method = "class", xval=5, minsplit =2,cp = 0.021)
```


11. Use rpart.plot to show your new tree model (the pruned tree). Show this with your preferred “type” and “extra” settings in rpart.plot.
```{r }
rpart.plot(model3,main='Tree model',type=2,extra=5)
```


12a. Create confusion matrices in R to assess the performance of your huge tree against your training and validation sets. How did it perform?
```{r }
library(caret)
#glimpse(train)
train$Attrition<-as.factor(train$Attrition)
model.pred<-predict(model1, train, type = "class")
confusionMatrix(model.pred, train$Attrition)
```

```{r }
valid$Attrition<-as.factor(valid$Attrition)
model.pred<-predict(model1, valid, type = "class")
confusionMatrix(model.pred, valid$Attrition)
```

My train model show 1 accuracy while my valid dataset shows 0.8 accuracy. Both datasets are so high that may cause overfitting problems.


12b. Now, create confusion matrices to assess your optimally-sized tree model (the one that you built after cross-validation). How was this optimally-sized model’s performance against the training and validation sets? What happened?
```{r }
model.pred<-predict(model3, train, type = "class")
confusionMatrix(model.pred, train$Attrition)
```


```{r }
model.pred<-predict(model3, valid, type = "class")
confusionMatrix(model.pred, valid$Attrition)
```
At this time my train model show 0.89  accuracy while my valid dataset shows 0.84 accuracy.The accuracy of the training set decreased while the accuracy for the valid set increased. The difference became smaller. 


12c. Why would it be reasonable to expect that the difference between training set accuracy and validation set accuracy would decrease when using a pruned tree?

Pruning tree reduces the size of decision trees since some of variables will be omitted. Decision trees are very likely to be overfitting so effective pruning can reduce such risk and improve the predictive accuracy. 


Task 2: Association rules

1. Describe “Groceries” by answering following questions:
● What is the class of “Groceries”?
● How many rows and columns does Groceries contain?
```{r }
#install.packages('arules')
library(arules)
library(datasets)
data(Groceries)
class(Groceries)
```

```{r }
inspect(head(Groceries,3))
```
```{r }
summary(Groceries)
```

This dataset belongs to transactions with 9835 rows and 169 columns. 


2. Generate an item frequency barplot for the top 15 grocery items in the dataset.
```{r }
library(ggplot2)
summary(Groceries@itemInfo$labels)
itemFrequency(Groceries[, 1:15])
Groceries@itemsetInfo
```

```{r }
itemFrequencyPlot(Groceries,topN = 15,col = rainbow(10),horiz = TRUE)
```

3. Now, create a subset of rules that contain your grocery item (you can find your item in the spreadsheet in Blackboard). Select any one rule with your item on the left-hand side, and any one rule with your item on the right-hand side,and explain them in the way you would explain them to your roommate (I’m assuming your roommate is a smart person who is unfamiliar with data mining).
```{r }
frequent <- eclat(Groceries,parameter = list(supp = 0.01, maxlen = 5))
inspect(head(sort(frequent)))
```

```{r }
rules <- apriori(Groceries,parameter = list(supp = 0.01,conf = 0.2,target = 'rules',minlen = 2, maxlen = 5))
inspect(head(sort(rules)))
```

```{r }
rules_left <- apriori(Groceries, parameter = list(support = 0.01, confidence=0.2,minlen=2),
                    appearance = list (default="rhs",lhs= "yogurt"), control = list (verbose=F))
inspect(rules_left)
```

```{r }
rules_right <- apriori(Groceries, parameter = list(support = 0.01, confidence=0.2,minlen=2),
                    appearance = list (default="lhs",rhs= "yogurt"),control = list (verbose=F))
inspect(rules_right)
```
Take right-hand side for example, rule is if a customer bought beef, then he may buy yogurt. Count shows that there are total 115 transactions following this rule. Support means that this bundle rule takes the total percentage of the total number of transactions. The likelyhood of first buying beef and then buy the yogurt is 0.012. Confidence means how often the customer will buy in such way.Here we can predict that it has 0.22 probability. Coverage means that this bundle appears in the whole data. We have 0.052 chance among all purchases to have yogurt in all the dataset.  Lift is the ratio of confidence to expected confidence. Here the rule is 1.598 at predicting the result than just assuming the result in the first place.



4. In a sentence or two, explain what meaning these rules might have for a store like Star Market. What could it do with this information?

Star market can use this information to do some sales bundle promotions. For example, food has an expiration date. If star market estimates that those food cannot quickly sold on time, they can use bundle method to make them sold out. Besides, star market can make some discounts for those slow-moving daily necessities to attract customers to think: since this item is in discount, we can also buy another one by the way.
Second, star market can maximumly increase its profit making full use of the rule method such as put them together in the shelves or so.



5. Using the plot() function in the arulesViz package, generate a scatter plot of any three rules involving your grocery item. Include a screenshot of your plot, along with the code you used to generate the plot.
```{r }
#install.packages('arulesViz')
library(arulesViz)
sp<-rules_left[1:3]
plot(sp)
```
This plot above is hard for me to tell the correlation with each other because there is only 2 dot on the plot. We need more data to see the relationships among variables.


6. Again using the plot() function in the arulesViz package, generate a plot for any three of your rules. This time, add two more arguments to the function: method="graph", engine="htmlwidget". What do you see now? Include a screenshot of your plot, along with the code you used to generate the plot. Describe your results in a sentence or two. In your answer, be sure to explain what the size of the circle, and shading of the circle, indicate. 
```{r }
sp1<-rules_left[1:3]
plot(sp,method="graph", engine="htmlwidget")
```

This plot very clearly shows three rules for yogurt. It points out that if a customer buys yogurt, he is more likely to buy tropical fruit or rolls/buns or other vegetables. For example, if this customer follows rule1: {yogurt}	=>	{tropical fruit}, he will have values with support=0.029, confidence=0.21, coverage=0.14, lift=2. The other two rules values can be checked in the rules table. 



